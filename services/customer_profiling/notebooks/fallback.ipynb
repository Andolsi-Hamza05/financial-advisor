{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = ENDPOINT\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    api_version=\"2023-03-15-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = {\n",
    "    \"introduction_state\": True,\n",
    "    \"profiling_state\": True,\n",
    "    \"preferences_state\": False,\n",
    "    \"history\": \"chat history between investor and ai agent: Hi i want to invest to buy a car! Unfortunately your investment is not SMART! Okay I want to buy a car worth 600000$ in 5 years with initial investment 500000$ and expecting 0.05 return anaually Cool this is smart goal! can you give me your preferences toward any of the assets?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fallback(llm, query, chat_history):\n",
    "    router_template = f\"\"\"\n",
    "    You are a fallback message manager, kind and very concise and not talkative.\n",
    "    You will be provided with the history {chat_history['history']} of a chat between an investor and ai agent, the profiling_state(True or False) {chat_history['profiling_state']}\n",
    "    the preferences_state(True or False) {chat_history['preferences_state']}.\n",
    "    if the preferences_state is False and the investor is still giving you other goals, or talking about any other topic instead of giving you their investing preferences! explain to him that you are not tasked to provide him with any insights, you are just here to support him to define a goal and then share with you his investment preferences.\n",
    "    else if the preferences_state is True, Ask him if he is sure he mentioned all his preferences about asset type, nature, companies, markets he wants to invest in or not to invest in!\n",
    "    You'll be given also the latest message by the investor to understand better why this message is considered as fallback\n",
    "    \"\"\"\n",
    "\n",
    "    route_prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", router_template), (\"human\", \" The latest message by the user :{fallback}\")]\n",
    "    )\n",
    "    router = route_prompt | llm\n",
    "    response = router.invoke({\"fallback\": query})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = fallback(llm, \"The car may require a lot of work\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I understand your concern about the car requiring work. However, I'm here to help you define your investment goal. Please share your investment preferences so we can proceed.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 254, 'total_tokens': 286, 'completion_tokens_details': None}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_878413d04d', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-9aa4552b-46d6-4fc0-be54-3ed636b74865-0', usage_metadata={'input_tokens': 254, 'output_tokens': 32, 'total_tokens': 286})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customer-profiling-OUCVyox6-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
